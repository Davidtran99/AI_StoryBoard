# âœ… Ollama Setup - HoÃ n Táº¥t Cáº¥u HÃ¬nh

## ÄÃ£ HoÃ n ThÃ nh:

### âœ… 1. Ollama Service
- Ollama Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t táº¡i `/opt/homebrew/bin/ollama`
- Ollama service Ä‘ang cháº¡y táº¡i `http://localhost:11434`
- Backend Docker container cÃ³ thá»ƒ truy cáº­p Ollama qua `host.docker.internal:11434`

### âœ… 2. Environment Variables (.env)
ÄÃ£ thÃªm vÃ o `.env`:
```
LOCAL_LLM_URL=http://host.docker.internal:11434/v1
LLM_MODEL_USE=localhost
```

### âœ… 3. Docker Configuration
- âœ… ÄÃ£ thÃªm `extra_hosts` vÃ o `docker-compose.yaml` Ä‘á»ƒ backend truy cáº­p Ollama trÃªn host
- âœ… Backend container Ä‘Ã£ restart vá»›i config má»›i
- âœ… Backend Ä‘ang cháº¡y vÃ  healthy

### âœ… 4. Code Integration
- âœ… RealChar Ä‘Ã£ cÃ³ `LocalLlm` class sáºµn, hoáº¡t Ä‘á»™ng vá»›i Ollama API
- âœ… `LocalLlm` sá»­ dá»¥ng OpenAI-compatible API format cá»§a Ollama

---

## âš ï¸ Cáº§n HoÃ n ThÃ nh (2 bÆ°á»›c):

### 1ï¸âƒ£ Pull Ollama Model

Ollama pull Ä‘ang gáº·p lá»—i authentication (cÃ³ thá»ƒ do network/proxy). Cáº§n pull model thá»§ cÃ´ng:

```bash
# Thá»­ cÃ¡c cÃ¡ch sau:
ollama pull llama3.2:3b

# Hoáº·c:
ollama pull mistral:7b

# Hoáº·c:
ollama pull phi3:mini

# Verify sau khi pull:
ollama list
```

**Náº¿u váº«n lá»—i:**
- Kiá»ƒm tra network/firewall
- Thá»­ restart Ollama: `killall ollama && ollama serve`
- Hoáº·c má»Ÿ Ollama app vÃ  pull model tá»« UI

### 2ï¸âƒ£ Speech-to-Text API Key

Backend cáº§n API key cho Whisper STT. Hiá»‡n Ä‘ang dÃ¹ng `OPENAI_WHISPER`.

**Option A:** ThÃªm OPENAI_API_KEY vÃ o `.env`
```bash
echo "OPENAI_API_KEY=your_key_here" >> .env
docker compose restart backend
```

**Option B:** Switch sang Google STT (cáº§n credentials file)
```bash
# Sá»­a .env:
SPEECH_TO_TEXT_USE=GOOGLE
GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json
docker compose restart backend
```

**Option C:** Chat text-only (khÃ´ng cáº§n STT/TTS)
- CÃ³ thá»ƒ chat qua text, chá»‰ cáº§n LLM hoáº¡t Ä‘á»™ng

---

## ğŸ§ª Test Sau Khi Pull Model:

1. **Verify model Ä‘Ã£ pull:**
```bash
ollama list
```

2. **Test Ollama API:**
```bash
curl http://localhost:11434/api/tags
# Should show your model in the list
```

3. **Test tá»« Docker container:**
```bash
docker compose exec backend curl -s http://host.docker.internal:11434/api/tags
```

4. **Restart backend Ä‘á»ƒ load config:**
```bash
docker compose restart backend
```

5. **Check logs:**
```bash
docker compose logs backend | grep -i llm
```

6. **Test chat:**
- Má»Ÿ http://localhost:3000
- Chá»n character
- Chá»n LLM model = "localhost" hoáº·c Ä‘á»ƒ auto-detect
- Gá»­i message vÃ  kiá»ƒm tra response

---

## ğŸ“Š Current Status:

| Component | Status | Notes |
|-----------|--------|-------|
| Ollama Service | âœ… Running | `http://localhost:11434` |
| Ollama Models | â³ Waiting | Cáº§n pull model |
| Backend Config | âœ… Complete | LOCAL_LLM_URL configured |
| Docker Network | âœ… Complete | extra_hosts configured |
| Backend Running | âœ… Healthy | Container up and running |
| STT | â³ Optional | Cáº§n API key hoáº·c switch config |
| LLM Integration | âœ… Ready | Äá»£i model |

---

## ğŸ¯ Next Steps:

1. **Pull model** (quan trá»ng nháº¥t)
2. **Config STT API key** (optional, cho voice)
3. **Test chat** táº¡i http://localhost:3000

---

## ğŸ” Troubleshooting:

**Náº¿u chat khÃ´ng response:**

1. Check Ollama model:
```bash
ollama list
# Náº¿u rá»—ng, cáº§n pull model
```

2. Check backend logs:
```bash
docker compose logs backend | grep -i "local\|llm\|error"
```

3. Test Ollama connection tá»« backend:
```bash
docker compose exec backend curl http://host.docker.internal:11434/api/tags
```

4. Verify env vars:
```bash
docker compose exec backend env | grep LOCAL_LLM
```

5. Restart náº¿u cáº§n:
```bash
docker compose restart backend
```

---

**Setup cÆ¡ báº£n Ä‘Ã£ xong! Chá»‰ cáº§n pull model lÃ  cÃ³ thá»ƒ chat.** ğŸš€

