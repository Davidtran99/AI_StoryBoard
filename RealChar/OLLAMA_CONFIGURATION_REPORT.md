# ğŸ¤– Ollama Configuration Report - RealChar Integration

## ğŸ“‹ Tá»•ng quan

TÃ i liá»‡u nÃ y mÃ´ táº£ cáº¥u hÃ¬nh tÃ­ch há»£p **Ollama** (Local LLM) vá»›i **RealChar** Ä‘á»ƒ sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh LLM cháº¡y local thay vÃ¬ API bÃªn ngoÃ i.

---

## ğŸ¯ Kiáº¿n trÃºc Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RealChar Application                     â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Backend (FastAPI)                      â”‚  â”‚
â”‚  â”‚  - Runs in Docker container                         â”‚  â”‚
â”‚  â”‚  - Port: 8000                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                   â”‚
â”‚                         â”‚ HTTP Request                      â”‚
â”‚                         â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         LocalLlm Class (local_llm.py)              â”‚  â”‚
â”‚  â”‚  - Uses LangChain ChatOpenAI                        â”‚  â”‚
â”‚  â”‚  - OpenAI-compatible API format                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                   â”‚
â”‚                         â”‚ via host.docker.internal         â”‚
â”‚                         â–¼                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ http://host.docker.internal:11434/v1
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Ollama Service (Host)                     â”‚
â”‚  - Runs on macOS/Linux host                                 â”‚
â”‚  - Port: 11434                                              â”‚
â”‚  - Provides OpenAI-compatible API                           â”‚
â”‚  - Models: llama3.2:3b, mistral, phi3, etc.                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Files Cáº¥u hÃ¬nh

### 1. Environment Variables (`.env`)

```env
# Local LLM Configuration (with OpenAI Compatible API)
LOCAL_LLM_URL=http://host.docker.internal:11434/v1

# Set model to localhost to use LocalLlm
LLM_MODEL_USE=localhost
```

**Giáº£i thÃ­ch:**
- `LOCAL_LLM_URL`: URL cá»§a Ollama API endpoint (OpenAI-compatible)
  - `host.docker.internal`: Äá»‹a chá»‰ Ä‘áº·c biá»‡t Ä‘á»ƒ Docker container truy cáº­p host machine
  - `11434`: Port máº·c Ä‘á»‹nh cá»§a Ollama
  - `/v1`: OpenAI-compatible API path
  
- `LLM_MODEL_USE`: Chá»‰ Ä‘á»‹nh sá»­ dá»¥ng local LLM thay vÃ¬ remote APIs

### 2. Docker Compose Configuration (`docker-compose.yaml`)

```yaml
services:
  backend:
    extra_hosts:
      - "host.docker.internal:host-gateway"  # âœ… Critical for Ollama access
```

**Giáº£i thÃ­ch:**
- `extra_hosts: host.docker.internal:host-gateway`: Cho phÃ©p container truy cáº­p services trÃªn host machine
  - Äiá»u nÃ y cáº§n thiáº¿t vÃ¬ Ollama cháº¡y trÃªn host, khÃ´ng pháº£i trong container
  - Docker tá»± Ä‘á»™ng map `host.docker.internal` Ä‘áº¿n IP cá»§a host

### 3. Backend LLM Implementation

#### 3.1 LocalLlm Class (`realtime_ai_character/llm/local_llm.py`)

```python
from langchain.chat_models import ChatOpenAI

class LocalLlm(LLM):
    def __init__(self, url):
        self.chat_open_ai = ChatOpenAI(
            model="Local LLM",
            temperature=0.5,
            streaming=True,
            openai_api_base=url,  # Ollama API URL
        )
```

**Äáº·c Ä‘iá»ƒm:**
- âœ… Sá»­ dá»¥ng LangChain `ChatOpenAI` vá»›i `openai_api_base` trá» Ä‘áº¿n Ollama
- âœ… Há»— trá»£ streaming responses
- âœ… TÃ­ch há»£p Chroma knowledge base cho context retrieval
- âœ… Temperature: 0.5 (cÃ¢n báº±ng giá»¯a sÃ¡ng táº¡o vÃ  chÃ­nh xÃ¡c)

#### 3.2 LLM Factory (`realtime_ai_character/llm/__init__.py`)

```python
if model == "localhost":
    local_llm_url = os.getenv("LOCAL_LLM_URL", "")
    if local_llm_url:
        from realtime_ai_character.llm.local_llm import LocalLlm
        return LocalLlm(url=local_llm_url)
```

---

## ğŸš€ Setup Instructions

### 1. Install Ollama

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. Start Ollama Service

```bash
ollama serve
# Verify: curl http://localhost:11434/api/tags
```

### 3. Pull Ollama Models

```bash
ollama pull llama3.2:3b    # Small, fast
ollama pull mistral         # Balanced
ollama pull phi3:mini       # Lightweight
ollama list                 # List models
```

### 4. Configure RealChar

**Step 1:** Update `.env` file:
```env
LOCAL_LLM_URL=http://host.docker.internal:11434/v1
LLM_MODEL_USE=localhost
```

**Step 2:** Verify `docker-compose.yaml` has:
```yaml
extra_hosts:
  - "host.docker.internal:host-gateway"
```

**Step 3:** Restart backend:
```bash
docker compose restart backend
```

---

## âœ… Testing

### 1. Test Ollama API Directly

```bash
curl http://localhost:11434/v1/models
```

### 2. Test from Docker Container

```bash
docker compose exec backend sh
curl http://host.docker.internal:11434/v1/models
```

---

## ğŸ”§ Troubleshooting

### Issue 1: Cannot connect to Ollama from Docker

**Solutions:**
1. Verify Ollama is running: `curl http://localhost:11434/api/tags`
2. Verify `extra_hosts` in docker-compose.yaml
3. Try using host IP directly in `.env`

### Issue 2: Model not found

**Solutions:**
```bash
ollama pull llama3.2:3b
ollama list
```

---

## ğŸ“Š Performance Comparison

| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| llama3.2:3b | 2.0GB | âš¡âš¡âš¡ Fast | â­â­â­ Good | Quick responses |
| mistral:7b | 4.1GB | âš¡âš¡ Medium | â­â­â­â­ Very Good | Balanced |
| phi3:mini | 2.3GB | âš¡âš¡âš¡ Fast | â­â­â­ Good | Lightweight |

**Recommendations:**
- **Development/Testing**: `llama3.2:3b` or `phi3:mini`
- **Production**: `mistral:7b`

---

## ğŸ” Security Notes

1. **Local Only**: Ollama cháº¡y local, khÃ´ng cáº§n internet
2. **No API Keys**: KhÃ´ng cáº§n API keys cho local LLM
3. **Docker Isolation**: RealChar backend cháº¡y trong container
4. **Network**: Chá»‰ truy cáº­p ná»™i bá»™ qua `host.docker.internal`

---

## âœ… Checklist Setup

- [ ] Ollama installed on host machine
- [ ] Ollama service running
- [ ] Ollama models pulled
- [ ] `.env` file configured with `LOCAL_LLM_URL`
- [ ] `.env` file configured with `LLM_MODEL_USE=localhost`
- [ ] `docker-compose.yaml` has `extra_hosts: host.docker.internal`
- [ ] Backend container restarted
- [ ] Connection tested from container
- [ ] Chat tested in RealChar web UI

---

## ğŸ“ Summary

**Ollama Integration vá»›i RealChar cho phÃ©p:**

âœ… Cháº¡y LLM hoÃ n toÃ n local, khÃ´ng cáº§n internet  
âœ… KhÃ´ng cáº§n API keys tá»« bÃªn thá»© ba  
âœ… Kiá»ƒm soÃ¡t hoÃ n toÃ n dá»¯ liá»‡u vÃ  models  
âœ… Privacy cao (dá»¯ liá»‡u khÃ´ng rá»i khá»i mÃ¡y)  

**Best Practice:**
- Development: DÃ¹ng small models (llama3.2:3b) cho speed
- Production: DÃ¹ng larger models (mistral:7b) cho quality

---

**Date:** 2024-11-02  
**Version:** RealChar + Ollama Integration  
**Status:** âœ… Production Ready
